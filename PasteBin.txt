Use cases:
- User enters block of text, gets randomly generated link that can be used to view the text
    - Link can expire
    - default behavior is link does not expire

- User can paste url and view the contents
    - In interview I would ask if relevant the format of the contents aka do they get a file to download etc

- User is anonymous

- Service tracks analytics like monthly visit

- Service deletes expired pastes

- Service has high availability (Non functional requirement)


Assumptions:
- 10 million users
- 10 million writes per month
- 100 reads per month
- 10:1 read to write ratio


Back of the envelope calculations:

- 1 KB content per paste
    - 1 byte per char, therefore 1000 chars per content
- tinyurl - 7 bytes aka 7 chars
- expiration_length: 4 bytes 
- created_at: 5 bytes
- paste_path: 255 bytes
- total: 1.27 KB

- therefore 1270 * 10,000,000 bytes per month = 12,700,000,000 = 12.7 gb


High level Design:

Components:
    - Web server to route traffic to APIs

    - API layer
        - Write
            - have logic to generate tinyurl
        - Read
            - check the database for record with tinyurl and return the contents

    - Object storage for content files

    - Db for storing the Content Metadata

    - Service for analytics


tinyurl logic:
- md5 hash
    - fixed size 128bits (32 chars)
    - Deterministic, same input always has same hash
    - Can have collisions
- Base62 encode md5 hash (only first 7 chars, not sure if that will guarantee uniqueness)
    - Has only Digits, Lowercase letters, and Uppercas letters so human readable and good for urls
    - also Deterministic


Scale the Design:
    - Add DNS
    
    - Add CDN for static content caching
    
    - API servers (Horizontal scaling)
        - Add loadbalancers to distribute traffic before hitting the webservers
            - Active - Active: multiple load balancers all working at same time
            - Active - Passive: one active load balancer, passives become active when previous active goes down
            - probably not relevant to this problem 

        - Add multple web servers
        - Add multiple API servers
        - All additional components and scaling add complexity but to ensure availability they are necessary additions
    
    - Cache
        - add a cache to the read API to reduce the amount of reads it needs to do to the db.

    - DB layer
        - horizontally scale using master slave replication to scale up read capabilities

    - analytics
        - develop system to read logs of the web server and count amount of hits on a specific url
        - could use MapReduce and store results in analytics database

